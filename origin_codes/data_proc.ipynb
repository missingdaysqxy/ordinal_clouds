{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load in...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from datetime import datetime\n",
    "\n",
    "print('Load in...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42164\n",
      "../../datasets/cloud/mode_2003/data_I2003.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42163 entries, 0 to 42162\n",
      "Data columns (total 26 columns):\n",
      "num_id                    42163 non-null int64\n",
      "Scene_ID                  42163 non-null object\n",
      "K-J                       42163 non-null int64\n",
      "Date                      42163 non-null object\n",
      "Time                      42163 non-null object\n",
      "SPOT                      42163 non-null int64\n",
      "HRV                       42163 non-null int64\n",
      "Mode                      42163 non-null object\n",
      "Upper_Left_Latitude       42163 non-null object\n",
      "Upper_Left_Longitude      42163 non-null object\n",
      "Upper_Right_Latitude      42163 non-null object\n",
      "Upper_Right_Longitude     42163 non-null object\n",
      "Scene_Centre_Latitude     42163 non-null object\n",
      "Scene_Centre_Longitude    42163 non-null object\n",
      "Lower_Left_Latitude       42163 non-null object\n",
      "Lower_Left_Longitude      42163 non-null object\n",
      "Lower_Right_Latitude      42163 non-null object\n",
      "Lower_Right_Longitude     42163 non-null object\n",
      "Cloud_Cover               42163 non-null object\n",
      "Max                       42163 non-null object\n",
      "Avg                       42163 non-null object\n",
      "Snow_Cover                42163 non-null int64\n",
      "Scene_Orient              42163 non-null float64\n",
      "Incident_Angle            42163 non-null float64\n",
      "Sun_Azimuth               18464 non-null float64\n",
      "Sun_Elevation             42163 non-null float64\n",
      "dtypes: float64(4), int64(5), object(17)\n",
      "memory usage: 8.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "BASEDIR = '../../datasets/cloud/mode_2003/'\n",
    "filenames = os.listdir(BASEDIR)\n",
    "print(len(filenames))\n",
    "csvname = os.path.join(BASEDIR, [it for it in filenames if '.csv' in it][0])\n",
    "print(csvname)\n",
    "df_train = pd.read_csv(csvname)\n",
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of images for training 8209\n",
      "Numbers of images for evaluation 33954\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42163 entries, 0 to 42162\n",
      "Data columns (total 27 columns):\n",
      "num_id                    42163 non-null int64\n",
      "Scene_ID                  42163 non-null object\n",
      "K-J                       42163 non-null int64\n",
      "Date                      42163 non-null object\n",
      "Time                      42163 non-null object\n",
      "SPOT                      42163 non-null int64\n",
      "HRV                       42163 non-null int64\n",
      "Mode                      42163 non-null object\n",
      "Upper_Left_Latitude       42163 non-null object\n",
      "Upper_Left_Longitude      42163 non-null object\n",
      "Upper_Right_Latitude      42163 non-null object\n",
      "Upper_Right_Longitude     42163 non-null object\n",
      "Scene_Centre_Latitude     42163 non-null object\n",
      "Scene_Centre_Longitude    42163 non-null object\n",
      "Lower_Left_Latitude       42163 non-null object\n",
      "Lower_Left_Longitude      42163 non-null object\n",
      "Lower_Right_Latitude      42163 non-null object\n",
      "Lower_Right_Longitude     42163 non-null object\n",
      "Cloud_Cover               42163 non-null object\n",
      "Max                       42163 non-null object\n",
      "Avg                       42163 non-null object\n",
      "Snow_Cover                42163 non-null int64\n",
      "Scene_Orient              42163 non-null float64\n",
      "Incident_Angle            42163 non-null float64\n",
      "Sun_Azimuth               18464 non-null float64\n",
      "Sun_Elevation             42163 non-null float64\n",
      "Train                     42163 non-null object\n",
      "dtypes: float64(4), int64(5), object(18)\n",
      "memory usage: 8.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def binary_sample(_, thresh=0.8):\n",
    "    return 'T' if np.random.uniform(0.0, 1.0) < thresh else 'F'\n",
    "    \n",
    "df_train['Train'] = df_train['num_id'].map(binary_sample)\n",
    "\n",
    "print('Numbers of images for training', df_train.loc[df_train['Train'] == 'F']['num_id'].count())\n",
    "print('Numbers of images for evaluation', df_train.loc[df_train['Train'] == 'T']['num_id'].count())\n",
    "\n",
    "df_train.to_csv(csvname)\n",
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train['num_id'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] 41804 images initialized as training data\n",
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      "WARNING:tensorflow:Variable conv_1/BatchNorm/beta missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable fc_3/weights missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable fc_3/biases missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable conv_2/BatchNorm/moving_mean missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable conv_2/BatchNorm/moving_variance missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable conv_2/BatchNorm/gamma missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable conv_1/BatchNorm/moving_variance missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable conv_2/BatchNorm/beta missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable conv_1/BatchNorm/moving_mean missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable conv_2/weights missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable conv_1/weights missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable Variable missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "WARNING:tensorflow:Variable conv_1/BatchNorm/gamma missing in checkpoint ./checkpoints/resnet_v1_152.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/resnet_v1_152.ckpt\n",
      "Model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v2, resnet_v1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from datetime import datetime\n",
    "ERROR_FLAG = 0\n",
    "\n",
    "pretrain_dir = './checkpoints/resnet_v1_152.ckpt'\n",
    "model_dir = './checkpoints/models/resnet_ordinal.model'\n",
    "data_dir = '../../datasets/cloud/mode_2004/'\n",
    "logdir = './logs/'\n",
    "optimizer = 'SGD'\n",
    "losstype = 'ordinal'\n",
    "is_training = True\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "pretrained = True\n",
    "\n",
    "def save(sess, model_dir, counter):\n",
    "\tif not os.path.exists(model_dir):\n",
    "\t\tos.makedirs(self.model_dir)\n",
    "\tsave_path = saver.save(sess, model_dir, global_step=counter)\n",
    "\tprint('MODEL RESTORED IN: ' + save_path)\n",
    "\n",
    "\n",
    "def load(sess, model_dir):\n",
    "\timport re\n",
    "\tprint(' [*] Reading checkpoints...')\n",
    "\tckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "\tsaver = tf.train.Saver(max_to_keep=1)\n",
    "\tif ckpt and ckpt.model_checkpoint_path:\n",
    "\t\tckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "\t\tsaver.restore(sess, model_dir + ckpt_name)\n",
    "\t\tcounter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "\t\tprint(\" [*] Success to read {}\".format(ckpt_name))\n",
    "\t\treturn counter\n",
    "\telse:\n",
    "\t\tprint(\" [*] Failed to find a checkpoint\")\n",
    "\t\treturn ERROR_FLAG\n",
    "\n",
    "def get_counter(model_dir):\n",
    "\timport re\n",
    "\tprint(' [*] Reading checkpoints...')\n",
    "\tckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "\tif ckpt and ckpt.model_checkpoint_path:\n",
    "\t\tckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "\t\tprint(\" [*] Success to read {}\".format(ckpt_name))\n",
    "\t\treturn int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "\telse:\n",
    "\t\tprint(\" [*] Failed to find a checkpoint\")\n",
    "\t\treturn ERROR_FLAG\n",
    "\n",
    "def init_reader(path=data_dir, batch_size=8, epoch=10, is_training=True):\n",
    "\tdef _parse_function(xs, ys):\n",
    "\t\tx_img_str = tf.read_file(xs)\n",
    "\t\tx_img_decoded = tf.image.convert_image_dtype(tf.image.decode_jpeg(x_img_str), tf.float32)\n",
    "\t\tx_img_resized = tf.image.resize_images(x_img_decoded, size=[512, 512],\n",
    "\t\t                                       method=tf.image.ResizeMethod.BILINEAR)\n",
    "\t\treturn x_img_resized, ys\n",
    "\n",
    "\t# Processing the image filenames\n",
    "\tfs = os.listdir(path)\n",
    "\tcsv_name = os.path.join(path, [it for it in fs if '.csv' in it][0])\n",
    "\n",
    "\t# Add one more column named \"Train\" to split the training set and validation set\n",
    "\tif is_training:\n",
    "\t\tframe = pd.read_csv(csv_name)\n",
    "\t\tframe = frame.loc[frame['Train'] == 'T']\n",
    "\t\tprint(' [*] {} images initialized as training data'.format(frame['num_id'].count()))\n",
    "\telse:\n",
    "\t\tframe = pd.read_csv(csv_name)\n",
    "\t\tframe = frame.loc[frame['Train'] == 'F']\n",
    "\t\tprint(' [*] {} images initialized as validation data'.format(frame['num_id'].count()))\n",
    "\n",
    "\tnum_idx = frame['num_id'].values.astype(str).tolist()\n",
    "\tt_names = [item + '.jpg' for item in num_idx]\n",
    "\tfile_names = [os.path.join(path, item) for item in t_names]\n",
    "\tlabels = frame['Cloud_Cover'].values.tolist()\n",
    "\tt_labels = [list('F'.join(item.split('*'))) for item in labels]\n",
    "\tfor it in range(len(t_labels)):\n",
    "\t\tt_labels[it] = list(map(lambda x: ord(x) - ord('A'), t_labels[it]))\n",
    "\t# Initialize as a tensorflow tensor object\n",
    "\tdata = tf.data.Dataset.from_tensor_slices((tf.constant(file_names),\n",
    "\t                                           tf.constant(t_labels)))\n",
    "\tdata = data.map(_parse_function)\n",
    "\tif is_training:\n",
    "\t\treturn data.shuffle(buffer_size=1024).batch(batch_size).repeat(epoch)\n",
    "\telse:\n",
    "\t\treturn data.batch_size(batch_size)\n",
    "    \n",
    "def init_loss(logits, labels, end_points=None, losstype='ordinal'):\n",
    "\tif end_points is not None:\n",
    "\t\t# Definition of binary network for better classification of \"*\"\n",
    "\t\t# The network has only 3 layers, with the front-end being resnet_v1_152/block3\n",
    "\t\t# See the graph in tensorboard for more detailed information\n",
    "\t\tconv_1 = slim.conv2d(end_points['resnet_v1_152/block4'], 64, [3, 3], scope='conv_1')\n",
    "\t\tconv_2 = slim.conv2d(conv_1, 1, [3, 3], scope='conv_2')\n",
    "\t\treshaped = tf.reshape(conv_2, [batch_size*8, -1], name='reshaped')\n",
    "\t\tbinary = slim.fully_connected(reshaped, 1, activation_fn=None, scope='fc_3')\n",
    "\t\tbinary_labels = tf.reshape(tf.cast(tf.equal(labels, 5), tf.float32), [-1, 1], name='binary_labels')\n",
    "\t\tbinary_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=binary_labels,\n",
    "\t\t                                                      logits=binary)\n",
    "\t\tbinary_loss = tf.reduce_mean(binary_loss, name='binary_loss')\n",
    "\t\n",
    "\t# Here we start our cross entropy loss definition\n",
    "\tif losstype == 'cross_entropy':\n",
    "\t\tloss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, \n",
    "\t\t                                                      logits=logits)\n",
    "\t\treturn tf.reduce_mean(loss, name='loss') + binary_loss, binary\n",
    "\telif losstype == 'ordinal':\n",
    "\t\timport math\n",
    "\t\tks = [np.arange(1, 7).astype(np.float32)[None, :] \\\n",
    "\t\t       for _ in range(batch_size * 8)]\n",
    "\t\tks = np.concatenate(ks, axis=0)\n",
    "\t\tkfac = [[math.factorial(it) for it in range(1, 7)] for _ in range(batch_size * 8)]\n",
    "\t\tkfac = np.array(kfac, dtype=np.float32)\n",
    "\t\tk_vector = tf.constant(ks, name='k_vector')\n",
    "\t\tk_factor = tf.constant(kfac, name='k_factor')\n",
    "\t\tsoftmaxed = tf.nn.softmax(logits, axis=-1, name='softmax')\n",
    "\t\tlog_exp = tf.log(softmaxed)\n",
    "\t\tpoisson = k_vector * log_exp - logits - tf.log(k_factor)\n",
    "\t\tloss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "\t\t                                                      logits=poisson)\n",
    "\t\treturn tf.reduce_mean(loss, name='loss') + binary_loss, binary\n",
    "\telse:\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "reader = init_reader(data_dir, batch_size=batch_size)\n",
    "batch_xs, batch_ys = reader.make_one_shot_iterator().get_next()\n",
    "# param batch_xs: shape [-1, 512, 512, 3] type tf.float32\n",
    "# param batch_ys: shape [-1] type tf.int32\n",
    "off_ws = [0, 0, 0, 0, 256, 256, 256, 256]\n",
    "off_hs = [0, 128, 256, 384, 0, 128, 256, 384]\n",
    "x_img_cuts = [tf.image.crop_to_bounding_box(batch_xs, hs, ws, 128, 256)\\\n",
    "                for hs, ws in zip(off_hs, off_ws)]\n",
    "batch_xs = tf.reshape(tf.concat(x_img_cuts, axis=0), [batch_size*8, 128, 256, 3])\n",
    "batch_ys = tf.reshape(batch_ys, [batch_size * 8])\n",
    "\n",
    "if is_training:\n",
    "    with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "        logits, end_points = resnet_v1.resnet_v1_152(batch_xs, num_classes=6, \n",
    "                                                                                                 is_training=True)\n",
    "        logits = tf.reshape(logits, [-1, 6], name='logits_2d')\n",
    "        prediction = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "        mAP = tf.reduce_mean(tf.cast(tf.equal(prediction, batch_ys), \n",
    "                                     dtype=tf.float32))\n",
    "        loss, _ = init_loss(logits, batch_ys, end_points=end_points, losstype=losstype)\n",
    "        mAP_sum = tf.summary.scalar('mAP', mAP)\n",
    "        loss_sum = tf.summary.scalar('loss', loss)\n",
    "        summaries = tf.summary.merge([mAP_sum, loss_sum])\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    counter = get_counter(model_dir)\n",
    "\n",
    "    # Exponential decay learning rate and optimizer configurations\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate, counter, \n",
    "                                               100, 0.98, staircase=True)\n",
    "    if 'SGD' in optimizer:\n",
    "        optim = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, \n",
    "                                                                          global_step=tf.Variable(counter))\n",
    "    elif 'Adam' in optimizer:\n",
    "        optim = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=tf.Variable(counter))\n",
    "    else:\n",
    "        optim = None\n",
    "        raise NotImplementedError\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if pretrained:\n",
    "        # Load the pretrained model given by TensorFlow official\n",
    "        exclusions = ['resnet_v1_152/logits', 'predictions']\n",
    "        resnet_except_logits = slim.get_variables_to_restore(exclude=exclusions)\n",
    "        init_fn = slim.assign_from_checkpoint_fn(pretrain_dir, resnet_except_logits,\n",
    "                                                 ignore_missing_vars=True)\n",
    "        init_fn(sess)\n",
    "        print('Model successfully loaded')\n",
    "    else:\n",
    "        # Load the model trained by ourselves\n",
    "        counter = load(sess, model_dir)\n",
    "\n",
    "# Ready to train\n",
    "# train(sess, optim, loss, summaries, FLAGS.loops, counter=counter)\n",
    "# print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.058282\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "sess.run(optim)\n",
    "lossval = sess.run(loss)\n",
    "print(lossval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
